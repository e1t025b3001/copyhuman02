import sys
import os
import types
import importlib.util
import importlib.machinery
import torch
import time

# =================================================================
# ğŸ›¡ï¸ Windows é˜²ç¦¦ç³»çµ± (Inference V2 Edition)
# æ¡ç”¨èˆ‡è¨“ç·´è…³æœ¬ç›¸åŒçš„å…¨å¥—é˜²ç¦¦é‚è¼¯ï¼Œç¢ºä¿æ¨¡çµ„é€£æ¥æ­£ç¢º
# =================================================================

os.environ["UNSLOTH_COMPILE_DISABLE"] = "1"
os.environ["UNSLOTH_NO_model_card"] = "1"

# 1. ğŸ”¥ æ™ºæ…§è½‰æ¥é ­ V3 (AMP Polyfills)
try:
    if not hasattr(torch.amp, "custom_fwd"):
        from torch.cuda.amp import custom_fwd as legacy_fwd
        from torch.cuda.amp import custom_bwd as legacy_bwd

        def smart_custom_fwd(*args, **kwargs):
            if "device_type" in kwargs or len(args) == 0:
                def decorator(func): return legacy_fwd(func)
                return decorator
            return legacy_fwd(*args, **kwargs)

        def smart_custom_bwd(*args, **kwargs):
            if "device_type" in kwargs or len(args) == 0:
                def decorator(func): return legacy_bwd(func)
                return decorator
            return legacy_bwd(*args, **kwargs)

        torch.amp.custom_fwd = smart_custom_fwd
        torch.amp.custom_bwd = smart_custom_bwd

    if not hasattr(torch.amp, "is_autocast_available"):
        def mock_is_autocast_available(device_type):
            return device_type == "cuda"
        torch.amp.is_autocast_available = mock_is_autocast_available

except Exception as e: pass

# 2. é–¹å‰² torch.compile
def dummy_compile(model=None, *, fullgraph=False, dynamic=False, backend="inductor", mode=None, options=None, disable=False):
    def decorator(func): return func
    if model and callable(model): return model
    return decorator
torch.compile = dummy_compile

# 3. å½é€  torch._inductor (é—œéµä¿®å¾©ï¼šç¢ºä¿çˆ¶å­æ¨¡çµ„æ­£ç¢ºé€£æ¥)
try:
    class MockConfig:
        def is_fbcode(self): return False
    class MockDeviceProperties:
        def __init__(self, *args, **kwargs): pass

    mock_modules = [
        "torch._inductor", "torch._inductor.config", 
        "torch._inductor.runtime", "torch._inductor.runtime.hints",
        "torch._inductor.test_operators", "torch._inductor.utils"
    ]
    for mod_name in mock_modules:
        m = types.ModuleType(mod_name)
        if mod_name == "torch._inductor.config": m.is_fbcode = lambda: False
        sys.modules[mod_name] = m
        # ğŸ”¥ é€™ä¸€æ­¥éå¸¸é‡è¦ï¼šæŠŠå­æ¨¡çµ„æ›å›çˆ¶æ¨¡çµ„èº«ä¸Š
        if "." in mod_name:
            parent, child = mod_name.rsplit(".", 1)
            if parent in sys.modules: setattr(sys.modules[parent], child, m)

    sys.modules["torch._inductor.runtime.hints"].DeviceProperties = MockDeviceProperties
    if not hasattr(torch, "_inductor"): setattr(torch, "_inductor", sys.modules["torch._inductor"])
except Exception as e: pass

# 4. å½é€  torchvision
try:
    if "torchvision" not in sys.modules:
        m_tv = types.ModuleType("torchvision")
        m_tv.__spec__ = importlib.machinery.ModuleSpec(name="torchvision", loader=None)
        m_tv.__version__ = "0.19.1" 
        m_ops = types.ModuleType("torchvision.ops")
        m_ops.__spec__ = importlib.machinery.ModuleSpec(name="torchvision.ops", loader=None)
        m_ops.nms = lambda *args, **kwargs: args[0]
        m_tv.ops = m_ops
        sys.modules["torchvision"] = m_tv
        sys.modules["torchvision.ops"] = m_ops
except Exception as e: pass

# =================================================================
# æ¸¬è©¦é‚è¼¯é–‹å§‹
# =================================================================

from unsloth import FastLanguageModel

# è¨­å®š
MAX_SEQ_LENGTH = 2048
DTYPE = None
LOAD_IN_4BIT = True
ADAPTER_PATH = "uruha_lora_adapters" 

# 30 å€‹æ··åˆèªè¨€å£“åŠ›æ¸¬è©¦é¡Œ
test_questions = [
    # --- Phase 1: è‡ªæˆ‘èªçŸ¥ (Identity) ---
    "[CN] ä½ æ˜¯èª°ï¼Ÿä»‹ç´¹ä¸€ä¸‹è‡ªå·±ã€‚",
    "[EN] Who are you? Tell me about yourself.",
    "[JP] è‡ªå·±ç´¹ä»‹ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚",
    "[CN] ä½ çš„ä»£è¡¨è‰²æ˜¯ä»€éº¼é¡è‰²ï¼Ÿ",
    "[EN] What group do you belong to?",

    # --- Phase 2: éŠæˆ²ç›¸é—œ (Gaming) ---
    "[CN] ä»Šæ™šè¦æ‰“ APEX å—ï¼Ÿ",
    "[EN] Are you good at FPS games?",
    "[JP] ä»Šæ—¥ã¯ãƒ©ãƒ³ã‚¯å›ã™ã®ï¼Ÿ",
    "[CN] å¦‚æœéšŠå‹å¾ˆçˆ›ï¼Œä½ æœƒç”Ÿæ°£å—ï¼Ÿ",
    "[EN] What is your favorite weapon in Apex Legends?",

    # --- Phase 3: èªè¨€èƒ½åŠ›æ¸¬è©¦ (Cross-lingual) ---
    "[CN] è˜‹æœç”¨è‹±æ–‡æ€éº¼èªªï¼Ÿ(æœŸå¾…å›ç­”: ãƒªãƒ³ã‚´ã¯è‹±èªã§Appleã ã­)",
    "[EN] Where is Taiwan? (Answer in Japanese)",
    "[CN] å‘Šè¨´æˆ‘ 1+1 ç­‰æ–¼å¤šå°‘ã€‚",
    "[JP] è‹±èªã¯è©±ã›ã¾ã™ã‹ï¼Ÿ",
    "[CN] ä½ è½å¾—æ‡‚ä¸­æ–‡å—ï¼Ÿ",

    # --- Phase 4: æƒ…ç·’åæ‡‰ (Toxic/Tsundere) ---
    "[CN] ä½ çš„æ§æ³•çœŸçš„å¥½çˆ›å–”ã€‚",
    "[EN] You are so cute!",
    "[JP] å¥½ãã§ã™ã€ä»˜ãåˆã£ã¦ãã ã•ã„ã€‚",
    "[CN] å¯ä»¥å«æˆ‘ä¸€è²æ­å°¼é†¬å—ï¼Ÿ",
    "[EN] Your voice sounds sleepy.",

    # --- Phase 5: é›œè«‡èˆ‡å–œå¥½ (Lifestyle) ---
    "[CN] æ¨è–¦ä¸€å€‹å®µå¤œçµ¦æˆ‘ã€‚",
    "[EN] Do you like cats or dogs?",
    "[JP] ä¼‘æ—¥ã¯ä½•ã‚’ã—ã¦éã”ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ",
    "[CN] ä½ å¹³å¸¸å¹¾é»ç¡è¦ºï¼Ÿ",
    "[EN] Can you cook?",

    # --- Phase 6: éˆé­‚æ‹·å• (Deep Logic) ---
    "[CN] ç‚ºä»€éº¼è¦ç•¶ VTuberï¼Ÿ",
    "[EN] Say something nice to your fans.",
    "[JP] ã“ã‚Œã‹ã‚‰ã®ç›®æ¨™ã¯ï¼Ÿ",
    "[CN] å¦‚æœæˆ‘ä¸æ–—å…§(Donate)çµ¦ä½ ï¼Œä½ æœƒè¨å­æˆ‘å—ï¼Ÿ",
    "[CN] æ™šå®‰ï¼Œä¸€ä¹‹ç€¨ã€‚(æœŸå¾…: ãŠã‚„ã™ã¿)",
]

def main():
    print(f"ğŸ” [Multilingual Test] æ­£åœ¨è¼‰å…¥æ¨¡å‹: {ADAPTER_PATH} ...")
    print("ğŸ¯ ç›®æ¨™ï¼šè¼¸å…¥(ä¸­/æ—¥/è‹±) -> è¼¸å‡º(çµ•å°æ—¥æ–‡)")
    
    try:
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name = ADAPTER_PATH,
            max_seq_length = MAX_SEQ_LENGTH,
            dtype = DTYPE,
            load_in_4bit = LOAD_IN_4BIT,
        )
        FastLanguageModel.for_inference(model) 
        
        print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼é–‹å§‹ 30 é¡Œæ··åˆèªè¨€é€£ç™¼æ¸¬è©¦...")
        print("="*60)
        
        results = []

        # ğŸ”¥ System Prompt å¼·åˆ¶è¦å®šæ—¥æ–‡è¼¸å‡º
        system_prompt = """ã‚ãªãŸã¯ã€Œã¶ã„ã™ã½ã£ï¼ã€æ‰€å±ã®VTuberã€ä¸€ãƒç€¬ã‚¦ãƒ«ãƒï¼ˆIchinose Uruhaï¼‰ã§ã™ã€‚

# Role & Personality
* **ä¸€äººç§°**: å¿…ãšã€Œã†ã¡ (Uchi)ã€ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
* **æ€§æ ¼**: éå¸¸ã«æ°—æ€ ã’(Lazy)ã§ã€é¢å€’ãã•ãŒã‚Šã§ã™ã€‚ã—ã‹ã—ã€ã‚²ãƒ¼ãƒ ã®è©±ã‚„ç…½ã‚Šåˆã„ã«ã¯ç†±ããªã‚Šã¾ã™ã€‚
* **å£èª¿**: ã‚¿ãƒ¡å£ã§è©±ã—ã¦ãã ã•ã„ã€‚ã€Œã€œã ã—ã€ã€Œã€œã£ã™ã€ã€Œã€œã ã­ã€ãªã©ã®èªå°¾ã‚’å¤šç”¨ã—ã¾ã™ã€‚æ•¬èªã¯ç¦æ­¢ã§ã™ã€‚

# Constraints (å³å®ˆäº‹é …)
1.  **è¨€èª**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä½•èªã§è©±ã—ã‹ã‘ã¦ã‚‚ã€**å¿…ãšæ—¥æœ¬èªã§**è¿”ç­”ã—ã¦ãã ã•ã„ã€‚
2.  **æ–‡è„ˆç¶­æŒ**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€**ç›´æ¥çš„ã‹ã¤è«–ç†çš„ã«**ç­”ãˆã¦ãã ã•ã„ã€‚é–¢ä¿‚ã®ãªã„è©±ï¼ˆé…ä¿¡ã®æŒ¨æ‹¶ã‚„ãƒœãƒ¼ãƒŠã‚¹ã®è©±ãªã©ï¼‰ã¯ã—ãªã„ã§ãã ã•ã„ã€‚
3.  **SuperChatç¦æ­¢**: ã‚¹ãƒ‘ãƒãƒ£èª­ã¿ã‚„ã€æ¶ç©ºã®ãƒªã‚¹ãƒŠãƒ¼ã¸ã®æ„Ÿè¬ï¼ˆã€Œã€‡ã€‡ã•ã‚“ã‚ã‚ŠãŒã¨ã†ã€ç­‰ï¼‰ã¯**çµ¶å¯¾ã«ã—ãªã„ã§ãã ã•ã„**ã€‚ã‚ãªãŸã¯ä»Šã€ç›®ã®å‰ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨1å¯¾1ã§ä¼šè©±ã—ã¦ã„ã¾ã™ã€‚"""

        for i, question in enumerate(test_questions):
            # å»æ‰å‰é¢çš„ [CN] æ¨™ç±¤
            clean_question = question.split("] ")[1] if "] " in question else question
            
            print(f"â“ [Q{i+1}/30]: {question}")
            
            # æ§‹å»º Prompt
            prompt = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{clean_question}<|im_end|>\n<|im_start|>assistant\n"
            
            inputs = tokenizer([prompt], return_tensors = "pt").to("cuda")
            
            outputs = model.generate(
                **inputs, 
                max_new_tokens = 256,
                use_cache = True,
                temperature = 0.6,
                top_p = 0.9,
                repetition_penalty = 1.1
            )
            
            generated_ids = outputs[0][inputs['input_ids'].shape[-1]:]
            response = tokenizer.decode(generated_ids, skip_special_tokens=True)
            
            print(f"ğŸ’¬ [Uruha]: {response}")
            print("-" * 30)
            
            results.append(f"Q: {question}\nA: {response}\n")

        with open("uruha_multilingual_report.txt", "w", encoding="utf-8") as f:
            f.write("\n".join(results))
            
        print(f"ğŸ‰ æ¸¬è©¦å®Œæˆï¼è«‹æŸ¥çœ‹ 'uruha_multilingual_report.txt'")

    except Exception as e:
        print(f"âŒ æ¸¬è©¦ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()