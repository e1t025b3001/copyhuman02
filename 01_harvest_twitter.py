import json
import time
import random
from DrissionPage import ChromiumPage, ChromiumOptions

# ç›®æ¨™ç¶²å€
TARGETS = [
    "https://twitter.com/uruhasub/with_replies", # å°å¸³å›è¦† (æœ€çœŸå¯¦)
    "https://twitter.com/uruhasub",              # å°å¸³ä¸»é 
    "https://twitter.com/uruha_ichinose"         # æœ¬å¸³
]
OUTPUT_FILE = "raw_tweets_v2.json"

def main():
    print("ğŸš€ åˆå§‹åŒ– DrissionPage (æ¯” Selenium æ›´å¼·çš„éš±å½¢çˆ¬èŸ²)...")
    
    # è¨­å®šç€è¦½å™¨é¸é …
    co = ChromiumOptions()
    # co.incognito() # å¦‚æœæƒ³è¦ç„¡ç—•æ¨¡å¼å¯ä»¥æ‰“é–‹ï¼Œä½†å»ºè­°ä¸è¦ï¼Œé€™æ¨£å¯ä»¥åƒä½ çš„ Chrome ç™»å…¥è³‡è¨Š
    
    # å•Ÿå‹•ç€è¦½å™¨
    page = ChromiumPage(co)
    
    collected_data = []
    
    try:
        # 1. å‰å¾€ç™»å…¥é é¢ (å¦‚æœå·²ç¶“ç™»å…¥éï¼Œé€™è£¡æœƒè‡ªå‹•è·³è½‰)
        page.get("https://twitter.com/home")
        
        print("\n" + "="*50)
        print("âš ï¸ ã€è«‹æ‰‹å‹•æ“ä½œã€‘")
        print("1. è«‹ç¢ºèªç€è¦½å™¨æ˜¯å¦å·²é–‹å•Ÿã€‚")
        print("2. å¦‚æœé‚„æ²’ç™»å…¥ Twitterï¼Œè«‹ç¾åœ¨æ‰‹å‹•ç™»å…¥ã€‚")
        print("3. ç¢ºèªçœ‹åˆ° Twitter é¦–é å¾Œï¼Œå›ä¾†é€™è£¡æŒ‰ [Enter] é–‹å§‹ã€‚")
        print("="*50 + "\n")
        input("ğŸ‘‰ æº–å‚™å¥½å¾Œè«‹æŒ‰ Enter...")

        # 2. é–‹å§‹çˆ¬å–
        for url in TARGETS:
            print(f"ğŸ” æ­£åœ¨å‰å¾€: {url}")
            page.get(url)
            time.sleep(3)
            
            # ç°¡å–®çš„é˜²å‘†ï¼šæª¢æŸ¥æ˜¯å¦çœŸçš„é€²å»äº†
            if "login" in page.url:
                print("âŒ åµæ¸¬åˆ°æœªç™»å…¥ï¼Œè«‹é‡æ–°ç™»å…¥å¾Œå†è©¦ã€‚")
                break

            consecutive_no_new = 0
            
            # æ¯å€‹é€£çµæ»¾å‹• 30 æ¬¡
            for i in range(30):
                print(f"   ğŸ“œ æ»¾å‹•ä¸­ ({i+1}/30)...")
                
                # æŠ“å–æ‰€æœ‰æ¨æ–‡å…ƒç´  (DrissionPage çš„èªæ³•å¾ˆç°¡æ½”)
                # é€™è£¡æŠ“å– data-testid ç‚º tweetText çš„ div
                tweets = page.eles('css:[data-testid="tweetText"]')
                
                new_count = 0
                for t in tweets:
                    txt = t.text.replace("\n", " ")
                    
                    # éæ¿¾åƒåœ¾è³‡è¨Š
                    if len(txt) > 3 and "http" not in txt and txt not in collected_data:
                        # ç°¡å–®éæ¿¾æ‰å–®ç´” @åˆ¥äºº çš„å›è¦† (æˆ‘å€‘è¦æœ‰å…§å®¹çš„)
                        if not txt.startswith("@"):
                            collected_data.append(txt)
                            new_count += 1
                            print(f"      âœ… æ”¶éŒ„: {txt[:20]}...")
                
                if new_count == 0:
                    consecutive_no_new += 1
                else:
                    consecutive_no_new = 0
                
                # å¦‚æœé€£çºŒ 3 æ¬¡æ²’æ–°æ±è¥¿ï¼Œå°±æ›ä¸‹ä¸€é 
                if consecutive_no_new >= 3:
                    print("ğŸš« ç„¡æ–°è³‡æ–™ï¼Œè·³è½‰ä¸‹ä¸€å€‹ç›®æ¨™ã€‚")
                    break

                # æ»¾å‹•åˆ°åº•éƒ¨
                page.scroll.to_bottom()
                
                # éš¨æ©Ÿç­‰å¾… (æ¨¡æ“¬äººé¡é–±è®€)
                time.sleep(random.uniform(2, 5))

    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    # å­˜æª”
    print(f"\nğŸ’¾ æ­£åœ¨å„²å­˜ {len(collected_data)} æ¢è³‡æ–™...")
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(collected_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… å®Œæˆï¼æª”æ¡ˆå·²å­˜ç‚º {OUTPUT_FILE}")

if __name__ == "__main__":
    main()