import sys
import os
import types
import importlib.util
import importlib.machinery
import torch

# =================================================================
# ğŸ›¡ï¸ Windows é˜²ç¦¦ç³»çµ± V14 (Checkpoint Edition)
# ç›®çš„ï¼šå¢åŠ  Epoch æ•¸è‡³ 3ï¼Œä¸¦åŠ å…¥è‡ªå‹•å­˜æª”åŠŸèƒ½ï¼Œé˜²æ­¢éæ“¬åˆå°è‡´ç™½å¿™ä¸€å ´
# =================================================================

# 1. å¼·åˆ¶é—œé–‰ç·¨è­¯åŠŸèƒ½
os.environ["UNSLOTH_COMPILE_DISABLE"] = "1"
os.environ["UNSLOTH_NO_model_card"] = "1"

# 2. ğŸ”¥ æ™ºæ…§è½‰æ¥é ­ V3 (AMP Polyfills)
try:
    if not hasattr(torch.amp, "custom_fwd"):
        from torch.cuda.amp import custom_fwd as legacy_fwd
        from torch.cuda.amp import custom_bwd as legacy_bwd

        def smart_custom_fwd(*args, **kwargs):
            if "device_type" in kwargs or len(args) == 0:
                def decorator(func): return legacy_fwd(func)
                return decorator
            return legacy_fwd(*args, **kwargs)

        def smart_custom_bwd(*args, **kwargs):
            if "device_type" in kwargs or len(args) == 0:
                def decorator(func): return legacy_bwd(func)
                return decorator
            return legacy_bwd(*args, **kwargs)

        torch.amp.custom_fwd = smart_custom_fwd
        torch.amp.custom_bwd = smart_custom_bwd

    if not hasattr(torch.amp, "is_autocast_available"):
        def mock_is_autocast_available(device_type):
            return device_type == "cuda"
        torch.amp.is_autocast_available = mock_is_autocast_available

except Exception as e:
    print(f"âš ï¸ AMP Patch Warning: {e}")

# 3. é–¹å‰² torch.compile
def dummy_compile(model=None, *, fullgraph=False, dynamic=False, backend="inductor", mode=None, options=None, disable=False):
    def decorator(func): return func
    if model and callable(model): return model
    return decorator
torch.compile = dummy_compile

# 4. å½é€  torch._inductor
try:
    class MockConfig:
        def is_fbcode(self): return False
    class MockDeviceProperties:
        def __init__(self, *args, **kwargs): pass

    mock_modules = [
        "torch._inductor", "torch._inductor.config", 
        "torch._inductor.runtime", "torch._inductor.runtime.hints",
        "torch._inductor.test_operators", "torch._inductor.utils"
    ]
    for mod_name in mock_modules:
        m = types.ModuleType(mod_name)
        if mod_name == "torch._inductor.config": m.is_fbcode = lambda: False
        sys.modules[mod_name] = m
        if "." in mod_name:
            parent, child = mod_name.rsplit(".", 1)
            if parent in sys.modules: setattr(sys.modules[parent], child, m)

    sys.modules["torch._inductor.runtime.hints"].DeviceProperties = MockDeviceProperties
    if not hasattr(torch, "_inductor"): setattr(torch, "_inductor", sys.modules["torch._inductor"])
except Exception as e: pass

# 5. å½é€  torchvision
try:
    if "torchvision" not in sys.modules:
        m_tv = types.ModuleType("torchvision")
        m_tv.__spec__ = importlib.machinery.ModuleSpec(name="torchvision", loader=None)
        m_tv.__version__ = "0.19.1" 
        m_ops = types.ModuleType("torchvision.ops")
        m_ops.__spec__ = importlib.machinery.ModuleSpec(name="torchvision.ops", loader=None)
        m_ops.nms = lambda *args, **kwargs: args[0]
        m_tv.ops = m_ops
        sys.modules["torchvision"] = m_tv
        sys.modules["torchvision.ops"] = m_ops
except Exception as e: pass

# =================================================================
# æ­£å¼ç¨‹å¼ç¢¼
# =================================================================

from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset

MAX_SEQ_LENGTH = 2048
DTYPE = None 
LOAD_IN_4BIT = True

def main():
    print("ğŸš€ [Training V14] 3 Epochs é€²éšè¨“ç·´æ¨¡å¼å•Ÿå‹•...")
    
    # è¼‰å…¥æ¨¡å‹
    print("ğŸ“¦ è¼‰å…¥ Qwen 2.5-7B (4-bit)...")
    try:
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name = "unsloth/Qwen2.5-7B-Instruct-bnb-4bit",
            max_seq_length = MAX_SEQ_LENGTH,
            dtype = DTYPE,
            load_in_4bit = LOAD_IN_4BIT,
        )
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return

    # é…ç½® LoRA
    print("ğŸ”§ é…ç½® LoRA...")
    model = FastLanguageModel.get_peft_model(
        model,
        r = 16, 
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj"],
        lora_alpha = 16,
        lora_dropout = 0, 
        bias = "none", 
        use_gradient_checkpointing = "unsloth", 
        random_state = 3407,
    )

    # è¼‰å…¥æ•¸æ“š
    print("ğŸ“‚ è¼‰å…¥æ•¸æ“šé›†...")
    dataset = load_dataset("json", data_files="uruha_final_train.json", split="train")

    def formatting_prompts_func(examples):
        instructions = examples["instruction"]
        inputs       = examples["input"]
        outputs      = examples["output"]
        texts = []
        for inst, inp, out in zip(instructions, inputs, outputs):
            text = f"<|im_start|>system\n{inst}<|im_end|>\n<|im_start|>user\n{inp}<|im_end|>\n<|im_start|>assistant\n{out}<|im_end|>"
            texts.append(text)
        return { "text" : texts }

    dataset = dataset.map(formatting_prompts_func, batched = True,)
    
    # è¨ˆç®—ç¸½æ­¥æ•¸
    total_steps_per_epoch = len(dataset) // (2 * 4) # batch_size 2 * grad_accum 4 = 8
    print(f"ğŸ”¥ é–‹å§‹è¨“ç·´ (è³‡æ–™é‡: {len(dataset)})")
    print(f"ğŸ’¡ é è¨ˆæ¯å€‹ Epoch æ­¥æ•¸: {total_steps_per_epoch}")
    print(f"ğŸ’¡ ç¸½ Epochs: 3 (ç¸½æ­¥æ•¸ç´„ {total_steps_per_epoch * 3})")
    
    trainer = SFTTrainer(
        model = model,
        tokenizer = tokenizer,
        train_dataset = dataset,
        dataset_text_field = "text",
        max_seq_length = MAX_SEQ_LENGTH,
        dataset_num_proc = 2,
        packing = False, 
        args = TrainingArguments(
            per_device_train_batch_size = 2,
            gradient_accumulation_steps = 4,
            warmup_steps = 5,
            
            # ğŸ”¥ é—œéµä¿®æ”¹ï¼š3 å€‹ Epochs
            num_train_epochs = 3, 
            
            learning_rate = 2e-4,
            fp16 = not torch.cuda.is_bf16_supported(),
            bf16 = torch.cuda.is_bf16_supported(),
            logging_steps = 10, 
            optim = "adamw_8bit",
            weight_decay = 0.01,
            lr_scheduler_type = "linear",
            seed = 3407,
            output_dir = "outputs",
            
            # ğŸ’¾ å®‰å…¨æ©Ÿåˆ¶ï¼šå­˜æª”ç­–ç•¥
            save_strategy = "steps",
            save_steps = 500,     # æ¯ 500 æ­¥å­˜ä¸€å€‹æª”
            save_total_limit = 3, # æœ€å¤šä¿ç•™ 3 å€‹å­˜æª”ï¼Œé¿å…ç¡¬ç¢Ÿçˆ†æ‰
        ),
    )

    trainer_stats = trainer.train()
    print(f"âœ… è¨“ç·´å®Œæˆï¼è€—æ™‚: {trainer_stats.metrics['train_runtime']} ç§’")
    
    print("ğŸ’¾ å„²å­˜æœ€çµ‚çµæœ...")
    model.save_pretrained("uruha_lora_adapters")
    tokenizer.save_pretrained("uruha_lora_adapters")
    print("ğŸ‰ è¨“ç·´çµæŸï¼è«‹åŸ·è¡Œ 05_merge.pyã€‚")

if __name__ == "__main__":
    main()